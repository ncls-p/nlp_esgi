{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43187f17-c723-41f0-8d6e-725d9c7b9f47",
   "metadata": {},
   "source": [
    "# TD2 part 1\n",
    "Dans ce TD, nous allons construire un RNN from scratch avec Pytorch, en suivant les étapes suivantes:\n",
    "\n",
    "- Sur un problème non-linéaire, nous allons construire un multi-layers perceptron\n",
    "- Sur un problème de texte, nous utiliserons pytorch pour optimiser un RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08035702-212d-480d-bf40-81fa151766c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1d0c6-6949-42b6-9414-72e1447220d1",
   "metadata": {},
   "source": [
    "# Problème simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed15a9-79b9-40b8-8717-313bef8e5a85",
   "metadata": {},
   "source": [
    "## Données exemples\n",
    "Nous allons construire des données $y = \\beta^* X + b$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab1c51-0dda-4209-bfc6-8613584a2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "a_star = 3.\n",
    "b_star = -3.\n",
    "noise_std = 1\n",
    "\n",
    "x = (np.random.rand(n, 1) - 0.5) * 4\n",
    "noise = np.random.normal(0, noise_std, (n, 1))\n",
    "y = a_star * x + b_star + noise\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "xvec = x\n",
    "plt.plot(x, y, 'o', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d475248-e907-49fe-bd63-db269c804589",
   "metadata": {},
   "source": [
    "### Constuire l'optimizer\n",
    "\n",
    "- Avec torch.nn.Parameter(), créer le paramètre beta qui sera optimisé. Les predictions du modèle sont égales à $\\beta \\times X$ \n",
    "- Avec torch.nn.MSELoss, déclarer la loss entre les prédictions et le résultat réel. loss.backward() pour mettre à jour les gradients\n",
    "- Avec torch.optim.Adam, déclarer un optimizer\n",
    "- Construisez la boucle qui, pour n_epochs, va reset les gradients, calculer la loss, mettre à jour les gradients et faire un pas pour optimiser beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c83e0-aff3-4f06-8ae1-f7fc8e4adac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b67ed-eb82-428c-b847-0c5070be3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot des predictions\n",
    "plt.plot(x, y, 'o', markersize=10)\n",
    "plt.plot(x, beta * x, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0d05e-cd55-4b03-b472-036840dc4b24",
   "metadata": {},
   "source": [
    "# Multi-layers perceptron\n",
    "\n",
    "## Données exemples\n",
    "\n",
    "Ici, on a un modèle $y = ax² + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6fc1c-2fd4-476e-be97-44ee3072e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "a_star = 3.\n",
    "b_star = -3.\n",
    "noise_std = 1\n",
    "\n",
    "x = (np.random.rand(n, 1) - 0.5) * 4\n",
    "noise = np.random.normal(0, noise_std, (n, 1))\n",
    "y = a_star * (x ** 2) + b_star + noise\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "xvec = x\n",
    "plt.plot(x, y, 'o', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f136309-aea7-4ed1-a0ba-d47666ef5f25",
   "metadata": {},
   "source": [
    "Nous allons construire un Pytorch modèle. Dans ce framework, on définit la fonction \"forward\" qui prend en argument les inputs et retourne les prédictions.\n",
    "\n",
    "A l'aide de torch.nn.Module et de différentes layers (torch.nn.Linear, torch.nn.ReLU), terminez la classe ci-dessous pour créer un multi-layers perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f40764-72e5-4c67-8ffc-28b4e9f9159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, layer_2_size):\n",
    "        super().__init__()\n",
    "        prev_size = input_size\n",
    "            \n",
    "        self.layer1 = torch.nn.Linear(input_size, layer_2_size)\n",
    "        self.layer2 = torch.nn.Linear(layer_2_size, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.layer_1(X)\n",
    "        X = torch.nn.functional.relu(X)\n",
    "        X = self.layer_2(X)\n",
    "        \n",
    "        return torch.nn.functional.relu(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d202ebd-5aa2-4269-adb3-da9d60f67ef3",
   "metadata": {},
   "source": [
    "En utilisant le travail précédent, faite une cellule utilisant un torch optimizer pour optimiser le MLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3957b-e9fe-4139-aa79-8f8c796ff272",
   "metadata": {},
   "source": [
    "# Problème de texte\n",
    "\n",
    "On a des données prénom -> pays d'origine.\n",
    "On veut utiliser un réseau de neurones qui, pour un nom, va prédire le pays d'origine en utilisant la suite de caractères."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50ed09-e90f-4dc6-8ee5-7d60ab49dae4",
   "metadata": {},
   "source": [
    "## Extraction des données\n",
    "\n",
    "Vous trouverez [ici](https://download.pytorch.org/tutorial/data.zip) un zip avec des fichiers \\[nationalité\\].txt contenant des prénoms pour chaque nationalité. <br/>\n",
    "Nous allons avoir un jeu de données pays -> prénoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f262124-648b-45f7-8160-75ef85ae77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code pour générer le jeu de données\n",
    "# En admettant que les fichiers ont été extraits dans data/raw/names/\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "path = Path(\"../data/raw/names/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fe9128-f799-4215-8502-365d6b3222d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_names(path):\n",
    "    country_to_names = {}\n",
    "    for file in path.iterdir():\n",
    "        if not file.name.endswith(\".txt\"):\n",
    "            continue\n",
    "            \n",
    "        with open(file) as f:\n",
    "            names = [unicodeToAscii(line.strip()) for line in f]\n",
    "        \n",
    "        country = file.stem\n",
    "        country_to_names[country] = names\n",
    "\n",
    "    return country_to_names\n",
    "\n",
    "# Handling ASCII stuff\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de2ea3d-6881-4548-b648-f8a9a2ffc533",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_names = read_names(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e12dc-3c4d-4aca-a7f9-586385386bda",
   "metadata": {},
   "source": [
    "## Transformation des prénoms en features\n",
    "\n",
    "On va one-hot encode les lettres des prénoms. <br/>\n",
    "Si on utilisait \"a\" ayant comme indice 0, \"b\" indice 1, ... \"z\" indice 25, <br/>\n",
    "le prénom \"abby\" aurait les index suivants [0, 1, 1, 24], <br/>\n",
    "ou la matrice: <br/>\n",
    "[ <br/>\n",
    "    \\[1, 0, ..., 0, 0\\] <br/>\n",
    "    \\[0, 1, ..., 0, 0\\] <br/>\n",
    "    \\[0, 1, ..., 0, 0\\] <br/>\n",
    "    \\[0, 0, ..., 1, 0\\] <br/>\n",
    "] <br/>\n",
    "\n",
    "Dans les prénoms, nous avons des caractères spéciaux (comme \"ö\", \"é\", etc), donc nous utiliserons la table ASCII pour traduire les lettres en indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead52645-92ff-4553-90bd-7f2246da2a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7bd56ab-d287-4215-943d-aea759331955",
   "metadata": {},
   "source": [
    "## Jouons un peu avec les RNN\n",
    "\n",
    "Le RNN (Recurrent neural network) est prévu pour prendre une série d'inputs et prédire un output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ec6ad-d1e1-409f-b9c6-77e1b604e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La table ascii a 128 caractères, une lettre one-hot encodée est donc un vecteur de taille 128\n",
    "# Les inputs seront de tailles 128\n",
    "model = torch.nn.RNN(input_size=128, hidden_size=1)\n",
    "# Hidden_size est la dimension de l'output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd69edb-aef7-4fe8-a8c7-a1cf9646497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La lettre \"a\" est l'indice 97 dans la table ascii\n",
    "letter_a = torch.zeros((1, 128))\n",
    "letter_a[0][97] = 1\n",
    "\n",
    "output, hidden = model(letter_a)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e494a-1ffb-46e2-a552-2b0e8273ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le prénom \"Abby\" est une suite de 4 lettres# La lettre \"a\" est l'indice 97 dans la table ascii\n",
    "name_abby = torch.zeros((1, 4, 128))\n",
    "name_abby[0][0][97] = 1\n",
    "name_abby[0][1][98] = 1\n",
    "name_abby[0][2][98] = 1\n",
    "name_abby[0][2][121] = 1\n",
    "\n",
    "output, hidden = model(name_abby)\n",
    "# On a un output pour chaque lettre. On considère que l'output de la dernière lettre est la nationalité à trouver\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b38e8-81ae-4583-b931-47b9eaa6b325",
   "metadata": {},
   "source": [
    "## Un RNN et une loss pour notre dataset\n",
    "Nous avons un problème de classification, nous allons utiliser la cross-entropy loss.\n",
    "\n",
    "Dans notre problème, nous avons 18 classes différentes (18 nationalités).\n",
    "Pour un prénom, notre réseau de neurones devra sortir la probabilité que le nom appartienne à chaque pays (un vecteur de taille 18, dont la somme fait 1).\n",
    "\n",
    "Notre réseau aura donc un output de taille 18, et nous appliquerons la fonction softmax pour en faire des probabilités (la somme des 18 outputs fera 1). <br/>\n",
    "Fonction softmax:\n",
    "\n",
    "$ \\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=0}^{K} e^{x_j}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a367336-ebe8-4175-9a5e-6e2168801ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.RNN(input_size=128, hidden_size=18)\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1024de-fc35-45bd-8dfb-399a04bd426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden = model(name_abby)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8cc62f-3d0a-4ad1-ae0f-75ff4a442ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape est 1 (un prénom), 4 (4 lettres), 18 (une valeur pour chacune des nationalités)\n",
    "\n",
    "# On utilise l'output de la dernière lettre\n",
    "output = output[:, -1, :]\n",
    "\n",
    "proba = torch.nn.functional.softmax(output)\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79c5c0-c54d-4f86-b334-512024cd54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour différents label (nationalité du prénom \"Abby\"), on a différentes cross-entropy loss\n",
    "y = torch.zeros((1))\n",
    "y = y.type(torch.LongTensor)\n",
    "y[0] = 1\n",
    "\n",
    "loss(proba, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea307a-49ca-4aeb-9eda-4b566291c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0] = 13\n",
    "\n",
    "loss(proba, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e384d8-ac24-42fa-9440-1a1299ba976f",
   "metadata": {},
   "source": [
    "# Votre tour\n",
    "\n",
    "Maintenant que nous avons des données, un modèle et une loss:\n",
    "- Séparez vos données en train & test\n",
    "- Faites une loop tirant au hasard des noms avec leur nationalité, calculez la sortie du réseau de neurones, sa loss, le gradient, et faites un step pour l'optimiser\n",
    "- Une fois que vous avez un modèle qui marche, testez d'autres architectures de réseaux de neurones pour avoir le meilleur résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55613360-9f05-47c8-878e-c7723e0354ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
